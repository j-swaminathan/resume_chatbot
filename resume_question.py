# Streamlit PDF QA Chatbot (FAISS + Sentence-Transformers + open-source LLM)
# ---------------------------------------------------------------
# Features
# - Upload a PDF, index it locally with FAISS
# - Ask questions; top-k relevant chunks are retrieved
# - Answer is generated by a small open-source LLM (default: google/flan-t5-small)
# - Runs fully local (no paid APIs)
# ----------------------------------------------------------------

import os
from typing import List, Tuple

import numpy as np
import streamlit as st
from sentence_transformers import SentenceTransformer
import faiss
from pypdf import PdfReader
import re
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM


# -----------------------------
# Caching & Loaders
# -----------------------------
@st.cache_resource(show_spinner=False)
def load_embedder(model_name: str = "sentence-transformers/all-MiniLM-L6-v2") -> SentenceTransformer:
    return SentenceTransformer(model_name)


@st.cache_resource(show_spinner=False)
def load_llm(model_name: str = "google/flan-t5-small") -> Tuple[AutoTokenizer, AutoModelForSeq2SeqLM, torch.device]:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()
    return tokenizer, model, device


# -----------------------------
# PDF -> Text -> Chunks
# -----------------------------

def extract_text_from_pdf(uploaded_file) -> str:
    reader = PdfReader(uploaded_file)
    texts = []
    for i, page in enumerate(reader.pages):
        try:
            txt = page.extract_text() or ""
        except Exception:
            txt = ""
        # Normalize whitespace
        txt = re.sub(r"\s+", " ", txt).strip()
        if txt:
            texts.append(f"[Page {i+1}]\n{txt}")
    return "\n\n".join(texts)


def chunk_text(text: str, max_chars: int = 800, overlap: int = 120) -> List[str]:
    # Simple semantic-ish splitter: split on sentences, then pack into windows
    sentences = re.split(r"(?<=[.!?])\s+", text)
    chunks, cur = [], ""
    for s in sentences:
        if len(cur) + len(s) + 1 <= max_chars:
            cur = (cur + " " + s).strip()
        else:
            if cur:
                chunks.append(cur)
            # start new window with overlap from end of previous chunk
            if overlap > 0 and chunks:
                tail = chunks[-1][-overlap:]
                cur = (tail + " " + s).strip()
            else:
                cur = s
    if cur:
        chunks.append(cur)
    return chunks


# -----------------------------
# FAISS Index
# -----------------------------

def build_faiss_index(chunks: List[str], embedder: SentenceTransformer):
    vecs = embedder.encode(chunks, convert_to_numpy=True, normalize_embeddings=True)
    dim = vecs.shape[1]
    index = faiss.IndexFlatIP(dim)  # cosine if vectors are normalized
    index.add(vecs.astype(np.float32))
    return index, vecs


def search(query: str, index, embedder: SentenceTransformer, chunks: List[str], top_k: int = 5) -> List[Tuple[int, float]]:
    q = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)
    scores, ids = index.search(q, top_k)
    results = [(int(i), float(s)) for i, s in zip(ids[0], scores[0])]
    return results


# -----------------------------
# LLM Answering
# -----------------------------

def build_prompt(question: str, retrieved_chunks: List[str]) -> str:
    context = "\n\n".join([f"Chunk {i+1}:\n{c}" for i, c in enumerate(retrieved_chunks)])
    return (
        "You are a helpful assistant that answers questions using ONLY the provided context.\n"
        "If the answer is not in the context, say you don't know.\n\n"
        f"Context:\n{context}\n\n"
        f"Question: {question}\n"
        "Answer:"
    )


def generate_answer(question: str, chunks: List[str], hits: List[Tuple[int, float]], tokenizer, model, device, max_new_tokens: int = 200) -> Tuple[str, List[str]]:
    retrieved_texts = [chunks[i] for i, _ in hits]
    prompt = build_prompt(question, retrieved_texts)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            num_beams=2,
            do_sample=False,
            early_stopping=True,
        )
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer.strip(), retrieved_texts


# -----------------------------
# Streamlit UI
# -----------------------------

st.set_page_config(page_title="Ask Your PDF", page_icon="ðŸ“„", layout="wide")

st.title("ðŸ“„ Ask Your PDF (Local RAG)")
st.caption("Upload a PDF and ask questions about its content. Uses FAISS + Sentence-Transformers + FLAN-T5 (open-source).")

with st.sidebar:
    st.header("Settings")
    embed_model_name = st.selectbox(
        "Embedding model",
        [
            "sentence-transformers/all-MiniLM-L6-v2",
            "sentence-transformers/all-MiniLM-L12-v2",
            "sentence-transformers/all-mpnet-base-v2",
        ],
        index=0,
    )
    llm_model_name = st.selectbox(
        "LLM model (Seq2Seq)",
        [
            "google/flan-t5-small",
            "google/flan-t5-base",
        ],
        index=0,
        help="Small = faster; Base = better quality but slower.",
    )
    top_k = st.slider("Top-K chunks", 2, 10, 5)
    max_new_tokens = st.slider("Max answer tokens", 50, 400, 200, 50)
    max_chars = st.slider("Chunk size (chars)", 400, 1500, 800, 50)
    overlap = st.slider("Chunk overlap (chars)", 0, 300, 120, 10)

# Session state containers
if "faiss_index" not in st.session_state:
    st.session_state.faiss_index = None
if "chunks" not in st.session_state:
    st.session_state.chunks = []
if "embedder_name" not in st.session_state:
    st.session_state.embedder_name = None
if "tokenizer" not in st.session_state:
    st.session_state.tokenizer = None
if "llm_model" not in st.session_state:
    st.session_state.llm_model = None
if "device" not in st.session_state:
    st.session_state.device = None

# Upload
uploaded_pdf = st.file_uploader("Upload a PDF", type=["pdf"]) 

col1, col2 = st.columns([1,1])

with col1:
    if st.button("ðŸ“š Index Document", use_container_width=True, type="primary"):
        if not uploaded_pdf:
            st.warning("Please upload a PDF first.")
        else:
            with st.spinner("Extracting text and building index..."):
                # Load models if needed
                if st.session_state.embedder_name != embed_model_name:
                    st.session_state.embedder = load_embedder(embed_model_name)
                    st.session_state.embedder_name = embed_model_name
                if st.session_state.tokenizer is None or st.session_state.llm_model is None or st.session_state.device is None or st.session_state.llm_model_name != llm_model_name:
                    tok, llm, device = load_llm(llm_model_name)
                    st.session_state.tokenizer = tok
                    st.session_state.llm_model = llm
                    st.session_state.device = device
                    st.session_state.llm_model_name = llm_model_name

                raw_text = extract_text_from_pdf(uploaded_pdf)
                if not raw_text:
                    st.error("Could not extract text from this PDF.")
                else:
                    st.session_state.chunks = chunk_text(raw_text, max_chars=max_chars, overlap=overlap)
                    index, _ = build_faiss_index(st.session_state.chunks, st.session_state.embedder)
                    st.session_state.faiss_index = index
                    st.success(f"Indexed {len(st.session_state.chunks)} chunks.")

with col2:
    if st.button("ðŸ§¹ Clear Index", use_container_width=True):
        st.session_state.faiss_index = None
        st.session_state.chunks = []
        st.success("Cleared index.")

st.divider()

# Chat interface
if st.session_state.faiss_index is None:
    st.info("Upload a PDF and click **Index Document** to get started.")
else:
    # History container
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"]) 

    user_q = st.chat_input("Ask a question about the PDF...")
    if user_q:
        st.session_state.messages.append({"role": "user", "content": user_q})
        with st.chat_message("user"):
            st.markdown(user_q)

        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                hits = search(user_q, st.session_state.faiss_index, st.session_state.embedder, st.session_state.chunks, top_k=top_k)
                answer, contexts = generate_answer(
                    user_q,
                    st.session_state.chunks,
                    hits,
                    st.session_state.tokenizer,
                    st.session_state.llm_model,
                    st.session_state.device,
                    max_new_tokens=max_new_tokens,
                )
                st.markdown(answer)
                with st.expander("Show sources"):
                    for i, (idx, score) in enumerate(hits, start=1):
                        st.markdown(f"**Chunk {i} (score {score:.3f}):**\n{st.session_state.chunks[idx][:500]}...")
        st.session_state.messages.append({"role": "assistant", "content": answer})

# Footer
st.caption("Built with Streamlit â€¢ FAISS â€¢ Sentence-Transformers â€¢ FLAN-T5")
